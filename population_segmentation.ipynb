{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Population Clusterization\n",
    "\n",
    "*Author*: Marcos Bressan\n",
    "\n",
    "The dataset used in this project was collected by the United States Census, and it provides information about characteristics of the demography of several counties in United States.\n",
    "\n",
    "OBS: Previously, I was training my model on census data available on https://www.census.gov/data.html. \n",
    "A few months later, I discovered an example for the same project on the [SameMaker blog](https://aws.amazon.com/blogs/machine-learning/analyze-us-census-data-for-population-segmentation-using-amazon-sagemaker/).\n",
    "It uses a public AWS's bucket to get the data from, which speeds up the download time and avoids more code for data pre-processing, since I'll be using Amazon AWS to train and deploy my model.\n",
    "\n",
    "## Objectives\n",
    "- Explore dataset\n",
    "- Pre-process/treat data\n",
    "- Apply Principal Components Analysis (PCA)\n",
    "- Feature Engineering / Dataset Transformation\n",
    "- Clusterization of transformed data\n",
    "- Visualization and interpretation of results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline \n",
    "# from sagemaker\n",
    "import boto3\n",
    "import sagemaker"
   ]
  },
  {
   "source": [
    "## Get data from S3 bucket\n",
    "\n",
    "As mentioned on [Sagemaker blog](https://aws.amazon.com/blogs/machine-learning/analyze-us-census-data-for-population-segmentation-using-amazon-sagemaker/), the data is availabke in `aws-ml-blog-sagemaker-census-segmentation` S3 bucket, easely downloadable using the `boto3` module.\n",
    "\n",
    "The desired file name is `Census_Data_for_SageMaker.csv`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('s3')\n",
    "\n",
    "dataset_bucket = 'aws-ml-blog-sagemaker-census-segmentation'\n",
    "dataset_filename = 'Census_Data_for_SageMaker.csv'\n",
    "\n",
    "dataset_object = client.get_object(Bucket=dataset_bucket, Key=dataset_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '9P9SCN5HDJ3Q6V1J',\n",
       "  'HostId': '8UqCBwwb6B6pIDDCCKcwDZzGNLjw0KSyqrLsiQNRBwMN1lW1z+AAOmTYPpW6HPY4FoMmPMhdiM0=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': '8UqCBwwb6B6pIDDCCKcwDZzGNLjw0KSyqrLsiQNRBwMN1lW1z+AAOmTYPpW6HPY4FoMmPMhdiM0=',\n",
       "   'x-amz-request-id': '9P9SCN5HDJ3Q6V1J',\n",
       "   'date': 'Tue, 17 Nov 2020 20:48:15 GMT',\n",
       "   'last-modified': 'Wed, 12 Sep 2018 15:13:37 GMT',\n",
       "   'etag': '\"066d37f43f7762f1eb409b1660fe9763\"',\n",
       "   'accept-ranges': 'bytes',\n",
       "   'content-type': 'text/csv',\n",
       "   'content-length': '613237',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 1},\n",
       " 'AcceptRanges': 'bytes',\n",
       " 'LastModified': datetime.datetime(2018, 9, 12, 15, 13, 37, tzinfo=tzutc()),\n",
       " 'ContentLength': 613237,\n",
       " 'ETag': '\"066d37f43f7762f1eb409b1660fe9763\"',\n",
       " 'ContentType': 'text/csv',\n",
       " 'Metadata': {},\n",
       " 'Body': <botocore.response.StreamingBody at 0x7f3ce23c4f90>}"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "# Exploring dataset object\n",
    "dataset_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_bytes = dataset_object[\"Body\"].read()\n",
    "bytes_stream = io.BytesIO(dataset_bytes)\n",
    "\n",
    "# Read from byte stream to create a dataframe\n",
    "counties_df = pd.read_csv(bytes_stream, header=0, delimiter=',') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   CensusId    State   County  TotalPop    Men  Women  Hispanic  White  Black  \\\n",
       "0      1001  Alabama  Autauga     55221  26745  28476       2.6   75.8   18.5   \n",
       "1      1003  Alabama  Baldwin    195121  95314  99807       4.5   83.1    9.5   \n",
       "2      1005  Alabama  Barbour     26932  14497  12435       4.6   46.2   46.7   \n",
       "3      1007  Alabama     Bibb     22604  12073  10531       2.2   74.5   21.4   \n",
       "4      1009  Alabama   Blount     57710  28512  29198       8.6   87.9    1.5   \n",
       "\n",
       "   Native  ...  Walk  OtherTransp  WorkAtHome  MeanCommute  Employed  \\\n",
       "0     0.4  ...   0.5          1.3         1.8         26.5     23986   \n",
       "1     0.6  ...   1.0          1.4         3.9         26.4     85953   \n",
       "2     0.2  ...   1.8          1.5         1.6         24.1      8597   \n",
       "3     0.4  ...   0.6          1.5         0.7         28.8      8294   \n",
       "4     0.3  ...   0.9          0.4         2.3         34.9     22189   \n",
       "\n",
       "   PrivateWork  PublicWork  SelfEmployed  FamilyWork  Unemployment  \n",
       "0         73.6        20.9           5.5         0.0           7.6  \n",
       "1         81.5        12.3           5.8         0.4           7.5  \n",
       "2         71.8        20.8           7.3         0.1          17.6  \n",
       "3         76.8        16.1           6.7         0.4           8.3  \n",
       "4         82.0        13.5           4.2         0.4           7.7  \n",
       "\n",
       "[5 rows x 37 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CensusId</th>\n      <th>State</th>\n      <th>County</th>\n      <th>TotalPop</th>\n      <th>Men</th>\n      <th>Women</th>\n      <th>Hispanic</th>\n      <th>White</th>\n      <th>Black</th>\n      <th>Native</th>\n      <th>...</th>\n      <th>Walk</th>\n      <th>OtherTransp</th>\n      <th>WorkAtHome</th>\n      <th>MeanCommute</th>\n      <th>Employed</th>\n      <th>PrivateWork</th>\n      <th>PublicWork</th>\n      <th>SelfEmployed</th>\n      <th>FamilyWork</th>\n      <th>Unemployment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1001</td>\n      <td>Alabama</td>\n      <td>Autauga</td>\n      <td>55221</td>\n      <td>26745</td>\n      <td>28476</td>\n      <td>2.6</td>\n      <td>75.8</td>\n      <td>18.5</td>\n      <td>0.4</td>\n      <td>...</td>\n      <td>0.5</td>\n      <td>1.3</td>\n      <td>1.8</td>\n      <td>26.5</td>\n      <td>23986</td>\n      <td>73.6</td>\n      <td>20.9</td>\n      <td>5.5</td>\n      <td>0.0</td>\n      <td>7.6</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1003</td>\n      <td>Alabama</td>\n      <td>Baldwin</td>\n      <td>195121</td>\n      <td>95314</td>\n      <td>99807</td>\n      <td>4.5</td>\n      <td>83.1</td>\n      <td>9.5</td>\n      <td>0.6</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>1.4</td>\n      <td>3.9</td>\n      <td>26.4</td>\n      <td>85953</td>\n      <td>81.5</td>\n      <td>12.3</td>\n      <td>5.8</td>\n      <td>0.4</td>\n      <td>7.5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1005</td>\n      <td>Alabama</td>\n      <td>Barbour</td>\n      <td>26932</td>\n      <td>14497</td>\n      <td>12435</td>\n      <td>4.6</td>\n      <td>46.2</td>\n      <td>46.7</td>\n      <td>0.2</td>\n      <td>...</td>\n      <td>1.8</td>\n      <td>1.5</td>\n      <td>1.6</td>\n      <td>24.1</td>\n      <td>8597</td>\n      <td>71.8</td>\n      <td>20.8</td>\n      <td>7.3</td>\n      <td>0.1</td>\n      <td>17.6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1007</td>\n      <td>Alabama</td>\n      <td>Bibb</td>\n      <td>22604</td>\n      <td>12073</td>\n      <td>10531</td>\n      <td>2.2</td>\n      <td>74.5</td>\n      <td>21.4</td>\n      <td>0.4</td>\n      <td>...</td>\n      <td>0.6</td>\n      <td>1.5</td>\n      <td>0.7</td>\n      <td>28.8</td>\n      <td>8294</td>\n      <td>76.8</td>\n      <td>16.1</td>\n      <td>6.7</td>\n      <td>0.4</td>\n      <td>8.3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1009</td>\n      <td>Alabama</td>\n      <td>Blount</td>\n      <td>57710</td>\n      <td>28512</td>\n      <td>29198</td>\n      <td>8.6</td>\n      <td>87.9</td>\n      <td>1.5</td>\n      <td>0.3</td>\n      <td>...</td>\n      <td>0.9</td>\n      <td>0.4</td>\n      <td>2.3</td>\n      <td>34.9</td>\n      <td>22189</td>\n      <td>82.0</td>\n      <td>13.5</td>\n      <td>4.2</td>\n      <td>0.4</td>\n      <td>7.7</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 37 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# Exploring dataframe rows\n",
    "counties_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "There are 37 columns: CensusId, State, County, TotalPop, Men, Women, Hispanic, White, Black, Native, Asian, Pacific, Citizen, Income, IncomeErr, IncomePerCap, IncomePerCapErr, Poverty, ChildPoverty, Professional, Service, Office, Construction, Production, Drive, Carpool, Transit, Walk, OtherTransp, WorkAtHome, MeanCommute, Employed, PrivateWork, PublicWork, SelfEmployed, FamilyWork, Unemployment.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           CensusId      TotalPop           Men         Women     Hispanic  \\\n",
       "count   3220.000000  3.220000e+03  3.220000e+03  3.220000e+03  3220.000000   \n",
       "mean   31393.605280  9.940935e+04  4.889694e+04  5.051241e+04    11.011522   \n",
       "std    16292.078954  3.193055e+05  1.566813e+05  1.626620e+05    19.241380   \n",
       "min     1001.000000  8.500000e+01  4.200000e+01  4.300000e+01     0.000000   \n",
       "25%    19032.500000  1.121800e+04  5.637250e+03  5.572000e+03     1.900000   \n",
       "50%    30024.000000  2.603500e+04  1.293200e+04  1.305700e+04     3.900000   \n",
       "75%    46105.500000  6.643050e+04  3.299275e+04  3.348750e+04     9.825000   \n",
       "max    72153.000000  1.003839e+07  4.945351e+06  5.093037e+06    99.900000   \n",
       "\n",
       "             White        Black       Native        Asian      Pacific  ...  \\\n",
       "count  3220.000000  3220.000000  3220.000000  3220.000000  3220.000000  ...   \n",
       "mean     75.428789     8.665497     1.723509     1.229068     0.082733  ...   \n",
       "std      22.932890    14.279122     7.253115     2.633079     0.734931  ...   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%      64.100000     0.500000     0.100000     0.200000     0.000000  ...   \n",
       "50%      84.100000     1.900000     0.300000     0.500000     0.000000  ...   \n",
       "75%      93.200000     9.600000     0.600000     1.200000     0.000000  ...   \n",
       "max      99.800000    85.900000    92.100000    41.600000    35.300000  ...   \n",
       "\n",
       "              Walk  OtherTransp   WorkAtHome  MeanCommute      Employed  \\\n",
       "count  3220.000000  3220.000000  3220.000000  3220.000000  3.220000e+03   \n",
       "mean      3.323509     1.612733     4.631770    23.278758  4.559352e+04   \n",
       "std       3.756096     1.670988     3.178772     5.600466  1.496995e+05   \n",
       "min       0.000000     0.000000     0.000000     4.900000  6.200000e+01   \n",
       "25%       1.400000     0.900000     2.700000    19.500000  4.550750e+03   \n",
       "50%       2.400000     1.300000     3.900000    23.000000  1.050800e+04   \n",
       "75%       4.000000     1.900000     5.600000    26.800000  2.863275e+04   \n",
       "max      71.200000    39.100000    37.200000    44.000000  4.635465e+06   \n",
       "\n",
       "       PrivateWork   PublicWork  SelfEmployed   FamilyWork  Unemployment  \n",
       "count  3220.000000  3220.000000   3220.000000  3220.000000   3220.000000  \n",
       "mean     74.219348    17.560870      7.931801     0.288106      8.094441  \n",
       "std       7.863188     6.510354      3.914974     0.455137      4.096114  \n",
       "min      25.000000     5.800000      0.000000     0.000000      0.000000  \n",
       "25%      70.500000    13.100000      5.400000     0.100000      5.500000  \n",
       "50%      75.700000    16.200000      6.900000     0.200000      7.600000  \n",
       "75%      79.700000    20.500000      9.400000     0.300000      9.900000  \n",
       "max      88.300000    66.200000     36.600000     9.800000     36.500000  \n",
       "\n",
       "[8 rows x 35 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CensusId</th>\n      <th>TotalPop</th>\n      <th>Men</th>\n      <th>Women</th>\n      <th>Hispanic</th>\n      <th>White</th>\n      <th>Black</th>\n      <th>Native</th>\n      <th>Asian</th>\n      <th>Pacific</th>\n      <th>...</th>\n      <th>Walk</th>\n      <th>OtherTransp</th>\n      <th>WorkAtHome</th>\n      <th>MeanCommute</th>\n      <th>Employed</th>\n      <th>PrivateWork</th>\n      <th>PublicWork</th>\n      <th>SelfEmployed</th>\n      <th>FamilyWork</th>\n      <th>Unemployment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>3220.000000</td>\n      <td>3.220000e+03</td>\n      <td>3.220000e+03</td>\n      <td>3.220000e+03</td>\n      <td>3220.000000</td>\n      <td>3220.000000</td>\n      <td>3220.000000</td>\n      <td>3220.000000</td>\n      <td>3220.000000</td>\n      <td>3220.000000</td>\n      <td>...</td>\n      <td>3220.000000</td>\n      <td>3220.000000</td>\n      <td>3220.000000</td>\n      <td>3220.000000</td>\n      <td>3.220000e+03</td>\n      <td>3220.000000</td>\n      <td>3220.000000</td>\n      <td>3220.000000</td>\n      <td>3220.000000</td>\n      <td>3220.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>31393.605280</td>\n      <td>9.940935e+04</td>\n      <td>4.889694e+04</td>\n      <td>5.051241e+04</td>\n      <td>11.011522</td>\n      <td>75.428789</td>\n      <td>8.665497</td>\n      <td>1.723509</td>\n      <td>1.229068</td>\n      <td>0.082733</td>\n      <td>...</td>\n      <td>3.323509</td>\n      <td>1.612733</td>\n      <td>4.631770</td>\n      <td>23.278758</td>\n      <td>4.559352e+04</td>\n      <td>74.219348</td>\n      <td>17.560870</td>\n      <td>7.931801</td>\n      <td>0.288106</td>\n      <td>8.094441</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>16292.078954</td>\n      <td>3.193055e+05</td>\n      <td>1.566813e+05</td>\n      <td>1.626620e+05</td>\n      <td>19.241380</td>\n      <td>22.932890</td>\n      <td>14.279122</td>\n      <td>7.253115</td>\n      <td>2.633079</td>\n      <td>0.734931</td>\n      <td>...</td>\n      <td>3.756096</td>\n      <td>1.670988</td>\n      <td>3.178772</td>\n      <td>5.600466</td>\n      <td>1.496995e+05</td>\n      <td>7.863188</td>\n      <td>6.510354</td>\n      <td>3.914974</td>\n      <td>0.455137</td>\n      <td>4.096114</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1001.000000</td>\n      <td>8.500000e+01</td>\n      <td>4.200000e+01</td>\n      <td>4.300000e+01</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>4.900000</td>\n      <td>6.200000e+01</td>\n      <td>25.000000</td>\n      <td>5.800000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>19032.500000</td>\n      <td>1.121800e+04</td>\n      <td>5.637250e+03</td>\n      <td>5.572000e+03</td>\n      <td>1.900000</td>\n      <td>64.100000</td>\n      <td>0.500000</td>\n      <td>0.100000</td>\n      <td>0.200000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>1.400000</td>\n      <td>0.900000</td>\n      <td>2.700000</td>\n      <td>19.500000</td>\n      <td>4.550750e+03</td>\n      <td>70.500000</td>\n      <td>13.100000</td>\n      <td>5.400000</td>\n      <td>0.100000</td>\n      <td>5.500000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>30024.000000</td>\n      <td>2.603500e+04</td>\n      <td>1.293200e+04</td>\n      <td>1.305700e+04</td>\n      <td>3.900000</td>\n      <td>84.100000</td>\n      <td>1.900000</td>\n      <td>0.300000</td>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>2.400000</td>\n      <td>1.300000</td>\n      <td>3.900000</td>\n      <td>23.000000</td>\n      <td>1.050800e+04</td>\n      <td>75.700000</td>\n      <td>16.200000</td>\n      <td>6.900000</td>\n      <td>0.200000</td>\n      <td>7.600000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>46105.500000</td>\n      <td>6.643050e+04</td>\n      <td>3.299275e+04</td>\n      <td>3.348750e+04</td>\n      <td>9.825000</td>\n      <td>93.200000</td>\n      <td>9.600000</td>\n      <td>0.600000</td>\n      <td>1.200000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>4.000000</td>\n      <td>1.900000</td>\n      <td>5.600000</td>\n      <td>26.800000</td>\n      <td>2.863275e+04</td>\n      <td>79.700000</td>\n      <td>20.500000</td>\n      <td>9.400000</td>\n      <td>0.300000</td>\n      <td>9.900000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>72153.000000</td>\n      <td>1.003839e+07</td>\n      <td>4.945351e+06</td>\n      <td>5.093037e+06</td>\n      <td>99.900000</td>\n      <td>99.800000</td>\n      <td>85.900000</td>\n      <td>92.100000</td>\n      <td>41.600000</td>\n      <td>35.300000</td>\n      <td>...</td>\n      <td>71.200000</td>\n      <td>39.100000</td>\n      <td>37.200000</td>\n      <td>44.000000</td>\n      <td>4.635465e+06</td>\n      <td>88.300000</td>\n      <td>66.200000</td>\n      <td>36.600000</td>\n      <td>9.800000</td>\n      <td>36.500000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 35 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "# Statistics\n",
    "print(f'There are {len(counties_df.columns)} columns: ' + ', '.join(counties_df.columns) + '.')\n",
    "\n",
    "counties_df.describe()"
   ]
  },
  {
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "Some of the columns represent numbers in absolute value, while others are shown in percentage. Some administrative data (such as `CensusId`) do not bring any important information to the model. Also, the dataset index must be defined to represent a meaningful and unique information, so the `State-County` pair will be used.\n",
    "\n",
    "### Pre-processing data\n",
    "#### Objectives:\n",
    "- Drop incomplete rows\n",
    "- Re-index dataframe to `State-County`\n",
    "- Drop useless / non-quantitative data (`State`, `County`, `CensusId`)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total rows before dropna: 3220, after dropna: 3218\nDropped rows: 2\n"
     ]
    }
   ],
   "source": [
    "clean_counties_df = counties_df.dropna()\n",
    "print(f'Total rows before dropna: {len(counties_df)}, after dropna: {len(clean_counties_df)}')\n",
    "\n",
    "print(f'Dropped rows: {len(counties_df)-len(clean_counties_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_counties_df.index = clean_counties_df['State'] + '-' + clean_counties_df['County']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_counties_df = clean_counties_df.drop(columns=['State', 'County', 'CensusId'])"
   ]
  },
  {
   "source": [
    "Remaining features are:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'clean_counties_df' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-17e0b7e0ef54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfeatures_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_counties_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Features: \\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mclean_counties_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clean_counties_df' is not defined"
     ]
    }
   ],
   "source": [
    "# features\n",
    "features_list = clean_counties_df.columns.values\n",
    "print('Features: \\n', features_list)\n",
    "clean_counties_df.head(5)"
   ]
  },
  {
   "source": [
    "## Data Visualization\n",
    "\n",
    "After data pre-processing, I want to visualize and explore the dataset further. For this, I plot histograms depicting the distributions of the datapoints. \n",
    "\n",
    "**The frequencies (y-axis) in the plots represent the number of counties that fall into each range of values (bins) for a specific quantitative column**."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of histograms to be generated\n",
    "columns_to_hist = ['Men', 'Women', 'Hispanic', 'White', 'Black', 'Native', 'Asian']\n",
    "n_bins = 30\n",
    "\n",
    "\n",
    "for column_name in columns_to_hist:\n",
    "    ax=plt.subplots(figsize=(6,3))\n",
    "    # get data by column_name and display a histogram\n",
    "    ax = plt.hist(clean_counties_df[column_name], bins=n_bins)\n",
    "    title=f\"Histogram of {column_name}\"\n",
    "    plt.title(title, fontsize=12)\n",
    "    plt.show()\n"
   ]
  },
  {
   "source": [
    "## Data normalization (Pre-training)\n",
    "\n",
    "Since the dataset columns present distinct range of values, at different scales, I use an MinMaxScaler to standardize numerical values."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# scale numerical features into a normalized range, 0-1\n",
    "# store them in this dataframe\n",
    "counties_scaled = clean_counties_df.copy()\n",
    "for column in counties_scaled:\n",
    "    counties_scaled[[column]] = MinMaxScaler().fit_transform(clean_counties_df[[column]])\n",
    "    \n",
    "counties_scaled.describe()"
   ]
  },
  {
   "source": [
    "## Get session, IAM role and bucket"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = sagemaker.Session() # SageMaker session\n",
    "\n",
    "role = sagemaker.get_execution_role() # IAM role for this session\n",
    "\n",
    "bucket_name = session.default_bucket() # Bucket"
   ]
  },
  {
   "source": [
    "## Principal Component Analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a folder name for this job's results\n",
    "prefix = 'counties'\n",
    "# Output path in S3\n",
    "output_path=f's3://{bucket_name}/{prefix}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a PCA model\n",
    "from sagemaker import PCA\n",
    "\n",
    "N_COMPONENTS=33\n",
    "\n",
    "pca_estimator = PCA(role=role,\n",
    "             train_instance_count=1,\n",
    "             train_instance_type='ml.c4.xlarge',\n",
    "             output_path=output_path,\n",
    "             num_components=N_COMPONENTS, \n",
    "             sagemaker_session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare RecordSet (required by SageMaker PCA Model)\n",
    "\n",
    "train_data_np = counties_scaled.values.astype('float32')\n",
    "\n",
    "formatted_train_data = pca_estimator.record_set(train_data_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "\n",
    "%%time\n",
    "\n",
    "pca_estimator.fit(formatted_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve job results from S3\n",
    "pca_job_name = 'pca-2020-09-25-20-39-46-297'\n",
    "\n",
    "path_to_model = os.path.join(prefix, training_job_name, 'output/model.tar.gz')\n",
    "\n",
    "# download and unzip model\n",
    "boto3.resource('s3').Bucket(bucket_name).download_file(model_key, 'model.tar.gz')\n",
    "# unzipping as model_algo-1\n",
    "os.system('tar -zxvf model.tar.gz')\n",
    "os.system('unzip model_algo-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MXNet model artifacts into a local MXNet\n",
    "import mxnet as mx\n",
    "\n",
    "pca_params = mx.ndarray.load('model_algo-1')"
   ]
  },
  {
   "source": [
    "### PCA Attributes\n",
    "\n",
    "Three types of model attributes are contained within the PCA model.\n",
    "\n",
    "- `mean`: The mean that was subtracted from a component in order to center it.\n",
    "- `v`: The makeup of the principal components; (same as ‘components_’ in an sklearn PCA model).\n",
    "- `s`: The singular values of the components for the PCA transformation. This does not exactly give the % variance from the original feature space, but can give the % variance from the projected feature space.\n",
    "\n",
    "We are only interested in `v` and `s`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the necessary attributes\n",
    "\n",
    "pca_v = pd.DataFrame(pca_params['v'].asnumpy())\n",
    "pca_s = pd.DataFrame(pca_params['s'].asnumpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The components are ordered in ascending order of s\n",
    "print(pca_s)"
   ]
  },
  {
   "source": [
    "### Dimensionality and Variance\n",
    "I want to keep the `n` most important features so that they totalize at least 80% of the original variance. Hopefully, a much smaller number of features will be necessary to cover most of the data variance, and those are the components to be picked in order to reduce the model dimensionality.\n",
    "\n",
    "The variance of a set of `n` components is given as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\sum_{i}^{n} {s_i}^2}{\\sum s^2}\n",
    "\\end{equation}"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the explained variance for the top n principal components\n",
    "def explained_variance(s, n_top_components):\n",
    "    '''Calculates the approx. data variance that n_top_components captures.\n",
    "       :param s: A dataframe of singular values for top components; \n",
    "           the top value is in the last row.\n",
    "       :param n_top_components: An integer, the number of top components to use.\n",
    "       :return: The expected data variance covered by the n_top_components.'''\n",
    "    \n",
    "    s2 = s * s\n",
    "    last_n_variance = s2.iloc[-n_top_components:].sum()[0]\n",
    "    total_variance = s2.iloc[:].sum()[0]\n",
    "    \n",
    "    return last_n_variance/total_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing \n",
    "n_top_components = 7 # select a value for the number of top components\n",
    "\n",
    "# calculate the explained variance\n",
    "exp_variance = explained_variance(s, n_top_components)\n",
    "print('Explained variance: ', exp_variance)"
   ]
  },
  {
   "source": [
    "### Composition of PCA components\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def display_component(v, features_list, component_num, n_weights=10):\n",
    "    \n",
    "    # get index of component (last row - component_num)\n",
    "    row_idx = N_COMPONENTS-component_num\n",
    "\n",
    "    # get the list of weights from a row in v, dataframe\n",
    "    v_1_row = v.iloc[:, row_idx]\n",
    "    v_1 = np.squeeze(v_1_row.values)\n",
    "\n",
    "    # match weights to features in counties_scaled dataframe, using list comporehension\n",
    "    comps = pd.DataFrame(list(zip(v_1, features_list)), \n",
    "                         columns=['weights', 'features'])\n",
    "\n",
    "    # we'll want to sort by the largest n_weights\n",
    "    # weights can be neg/pos and we'll sort by magnitude\n",
    "    comps['abs_weights']=comps['weights'].apply(lambda x: np.abs(x))\n",
    "    sorted_weight_data = comps.sort_values('abs_weights', ascending=False).head(n_weights)\n",
    "\n",
    "    # display using seaborn\n",
    "    ax=plt.subplots(figsize=(10,6))\n",
    "    ax=sns.barplot(data=sorted_weight_data, \n",
    "                   x=\"weights\", \n",
    "                   y=\"features\", \n",
    "                   palette=\"Blues_d\")\n",
    "    ax.set_title(\"PCA Component Makeup, Component #\" + str(component_num))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show composition of a given component\n",
    "num=1\n",
    "display_component(pca_v, counties_scaled.columns.values, component_num=num, n_weights=15)"
   ]
  },
  {
   "source": [
    "## Deploying PCA to evaluate dataset\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# this takes a little while, around 7mins\n",
    "pca_predictor = pca_SM.deploy(initial_instance_count=1,instance_type='ml.t2.medium')"
   ]
  },
  {
   "source": [
    "We can pass the original, numpy dataset to the model and transform the data using the model we created. Then we can take the largest n components to reduce the dimensionality of our data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass np train data to the PCA model\n",
    "train_pca = pca_predictor.predict(train_data_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dimensionality-reduced data\n",
    "def create_transformed_df(train_pca, counties_scaled, n_top_components):\n",
    "    ''' Return a dataframe of data points with component features. \n",
    "        The dataframe should be indexed by State-County and contain component values.\n",
    "        :param train_pca: A list of pca training data, returned by a PCA model.\n",
    "        :param counties_scaled: A dataframe of normalized, original features.\n",
    "        :param n_top_components: An integer, the number of top components to use.\n",
    "        :return: A dataframe, indexed by State-County, with n_top_component values as columns.        \n",
    "     '''\n",
    "    # create a dataframe of component features, indexed by State-County\n",
    "    new_df = pd.DataFrame()\n",
    "    \n",
    "    # your code here\n",
    "    for row in range(len(train_pca)):\n",
    "        for col in range(n_top_components):\n",
    "            new_df.loc[counties_scaled.index[row], f'c_{col+1}'] = train_pca[row].label['projection'].float32_tensor.values[-col-1]\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specify top n\n",
    "top_n = n_top_components\n",
    "\n",
    "# call your function and create a new dataframe\n",
    "counties_transformed = create_transformed_df(train_pca, counties_scaled, n_top_components=top_n)\n",
    "\n",
    "\n",
    "# print result\n",
    "counties_transformed.head()"
   ]
  },
  {
   "source": [
    "### Delete PCA endpoint\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete predictor endpoint\n",
    "session.delete_endpoint(pca_predictor.endpoint)"
   ]
  },
  {
   "source": [
    "---\n",
    "## Population Segmentation\n",
    "\n",
    "The transformed model now should be used to feed our unsupervised learning model. \n",
    "For the clusterization, I'll apply the `K-means` clusterization."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a KMeans estimator\n",
    "from sagemaker import KMeans\n",
    "estimator = KMeans(\n",
    "    role, \n",
    "    train_instance_count=1, \n",
    "    train_instance_type='ml.c4.xlarge', \n",
    "    k=8,\n",
    "    sagemaker_session=session, \n",
    "    output_path=f's3://{bucket_name}/{prefix}/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RecordSet\n",
    "\n",
    "train_data_np = counties_transformed.values.astype('float32')\n",
    "\n",
    "# convert to RecordSet format\n",
    "train_data_rs = estimator.record_set(train_data_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "%%time\n",
    "estimator.fit(records=train_data_rs, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy model\n",
    "%%time\n",
    "kmeans_predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.t2.medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the predicted clusters for all the kmeans training data\n",
    "cluster_info = kmeans_predictor.predict(train_data_np)"
   ]
  },
  {
   "source": [
    "## Exploring clusters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print cluster info for a given data point\n",
    "data_idx = 0\n",
    "\n",
    "print('County is: ', counties_transformed.index[data_idx])\n",
    "print()\n",
    "print(cluster_info[data_idx])"
   ]
  },
  {
   "source": [
    "### Distribution of data over clusters\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all cluster labels\n",
    "cluster_labels = [c.label['closest_cluster'].float32_tensor.values[0] for c in cluster_info]\n",
    "\n",
    "# count up the points in each cluster\n",
    "cluster_df = pd.DataFrame(cluster_labels)[0].value_counts()\n",
    "\n",
    "print(cluster_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete endpoint\n",
    "session.delete_endpoint(kmeans_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "---\n",
    "# Model Attributes and Explainability\n",
    "\n",
    "Explaining the result of the modeling is an important step in making use of our analysis. By combining PCA and k-means, and the information contained in the model attributes within a SageMaker trained model, we can learn about a population and remark on some patterns we've found, based on the data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and unzip the kmeans model file\n",
    "training_job_name='kmeans-2020-09-30-15-26-50-572'\n",
    "\n",
    "# where the model is saved, by default\n",
    "model_key = os.path.join(prefix, training_job_name, 'output/model.tar.gz')\n",
    "print(model_key)\n",
    "\n",
    "# download and unzip model\n",
    "boto3.resource('s3').Bucket(bucket_name).download_file(model_key, 'model.tar.gz')\n",
    "\n",
    "# unzipping as model_algo-1\n",
    "os.system('tar -zxvf model.tar.gz')\n",
    "os.system('unzip model_algo-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the trained kmeans params using mxnet\n",
    "kmeans_model_params = mx.ndarray.load('model_algo-1')\n",
    "\n",
    "# what are the params\n",
    "print(kmeans_model_params)"
   ]
  },
  {
   "source": [
    "There is only 1 set of model parameters contained within the k-means model: the cluster centroid locations in PCA-transformed, component space.\n",
    "\n",
    "- **centroids**: The location of the centers of each cluster in component space, identified by the k-means algorithm."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the centroids\n",
    "cluster_centroids=pd.DataFrame(kmeans_model_params[0].asnumpy())\n",
    "cluster_centroids.columns=counties_transformed.columns\n",
    "\n",
    "display(cluster_centroids)"
   ]
  },
  {
   "source": [
    "### Visualization of Centroids"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a heatmap in component space, using the seaborn library\n",
    "plt.figure(figsize = (12,9))\n",
    "ax = sns.heatmap(cluster_centroids.T, cmap = 'YlGnBu')\n",
    "ax.set_xlabel(\"Cluster\")\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.xticks(fontsize = 16)\n",
    "ax.set_title(\"Attribute Value by Centroid\")\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "### Natural Groupings"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a 'labels' column to the dataframe\n",
    "counties_transformed['labels']=list(map(int, cluster_labels))\n",
    "\n",
    "# sort by cluster label 0-6\n",
    "sorted_counties = counties_transformed.sort_values('labels', ascending=True)\n",
    "# view some pts in cluster 0\n",
    "sorted_counties.head(20)"
   ]
  },
  {
   "source": [
    "---\n",
    "\n",
    "\n",
    "## Delete buckets  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "usage: aws [options] <command> <subcommand> [<subcommand> ...] [parameters]\nTo see help text, you can run:\n\n  aws help\n  aws <command> help\n  aws <command> <subcommand> help\naws: error: the following arguments are required: path\n"
     ]
    }
   ],
   "source": [
    "!aws s3 rb $bucket_name --force"
   ]
  }
 ]
}